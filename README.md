# BashLLM
What if you could talk to bash?

## Getting Started
```sh
# Clone the repo
git clone https://github.com/sceptyre/bash-llm
cd bash-llm

# Install dependencies
python3 -m pip install -r requirements.txt

# Run BashLLM
python3 -m bash_llm
```

## Usage 
`python3 -m bash_llm --help`

## Config
`./config.json`
```json
{
    "always_sudo": false,
    "prompt_before_execute": true,
    "weak_model": "Qwen/Qwen2.5-Coder-3B-Instruct",
    "strong_model": "Qwen/Qwen2.5-Coder-14B-Instruct"
}
```

## Example
````
$ python3 -m bash_llm  
Chat: what time is it?  
Planner Evaluation:  
1. `date +"%H:%M:%S"`
Coder Response:
```bash
date +"%H:%M:%S"
```
Execute coder output? (y/n)y
Invoking: bash -c sudo bash model_response.sh
00:53:22
````

## Thought Process
Currently requests go through 2 stages. A planner and a coder. Long term, there may be more like 4 as a sort of checks and balances system and also to create a bit more of a logical loop for decision making.

### The Planner
This portion runs on the "small brain". This piece only has one task and it is basically inference. It translates user input into a list of tasks to be handed to the coder to build. This is to eventually mitigate issues of information the coder may not have up to date information on (ex. install xyz software). This piece sometimes generates commands that the coder blindly trusts which needs further review. The coder then is provided the user input and the tactical plan for execution and generates a bash script to perform the planned actions


### The Coder
This portion runs on the "big brain". This has been set up to explicitly output just bash code. Sometimes some weird formats bleed through the cracks and handling the weird cases is still a wip. Currently the system handles a lot of basic questions fairly reasonably. Things like 
- `What processes are running?`
- `How much storage space is free?`
- `What time is it?`
- `What packages are installed?`
- `Init a git repo in the current directory`

### Execution
Execution happens with no check against the coder. There is an option in the config file to enable `always_sudo`. This is because sometimes the generated code forgets misses a sudo and an apt-get or other operation will just fail due to missing permissions. There is also an option for `prompt_before_execute` that'll allow bash_llm to run without the y/n prompt.

## Next Steps
- Establish a system to collect up to date information to be provided to the coder.
- Reduce the level of trust the coder has in commands listed in the planner's plan.
- Introduce a reviewer to operate as a somewhat firewall against anything generated by the coder.
- Introduce a feedback loop of some sort to provide context to the coder and planners the current state of the host system and potential outputs of previous outputs without a full chat history.
- Logging and storing of user prompts, commands, and results that can be recovered later on to be used for context almost like a file archive or notes. Maybe a historian persona?